# LLM Manager

<p align="center">
  <img src="https://img.shields.io/badge/version-1.0.0-blue.svg" alt="Version">
  <img src="https://img.shields.io/badge/python-3.10+-green.svg" alt="Python">
</p>

A resilient, production-ready wrapper for multiple LLM providers with automatic failover.

## Features

- ğŸ”„ **Automatic Failover** - Primary â†’ Secondary â†’ Local
- ğŸŒ **Multi-Provider** - Gemini, OpenAI, Ollama, Groq support
- âš¡ **Async/Await** - Full async support
- ğŸ“ **Comprehensive Logging** - Every failure logged
- ğŸ›¡ï¸ **Safe Fallbacks** - Local cache + hardcoded responses
- âš™ï¸ **Environment-Driven** - .env configuration
- ğŸ” **Retry Logic** - Exponential backoff on 5xx errors

## Quick Start

```bash
# Install dependencies
pip install python-dotenv aiohttp google-generativeai openai groq

# Copy config
cp .env.example .env
# Add your API keys

# Use in code
from llm_manager import LLMManager

manager = LLMManager()
await manager.initialize()

response = await manager.generate_text("Hello!")
print(response.text)
```

## Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| PRIMARY_PROVIDER | First LLM to try | gemini |
| SECONDARY_PROVIDER | Fallback LLM | groq |
| LOCAL_PROVIDER_URL | Ollama URL | http://localhost:11434 |
| FALLBACK_STRATEGY | Failover order | ordered |

## Architecture

```
LLMManager
â”œâ”€â”€ GeminiProvider    (Google Gemini)
â”œâ”€â”€ OpenAIProvider   (GPT-4/3.5)
â”œâ”€â”€ OllamaProvider   (Local models)
â”œâ”€â”€ GroqProvider     (Fast inference)
â””â”€â”€ SafeFallbackCache
```

## License

MIT
